---
title: "Actblue Markov Analysis"
author: "Tomer Eldor"
date: "2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Political Donations Lifetime Value Analysis using Markovian States of Engagement
#### Optimizing efficiency of data transformations with data.table

# Setup 
First, let's install if needed and load the required libraries.
I'll be working mainly with "data.table" library, since it is both more computationally efficient, and very convenient to work with. I highly recommend learning that library and speeding up your future R work.
```{r echo=FALSE}
# Loading Libraries
install.packages(c('data.table','RODBC','tidyr','plyr', "dplyr", "ggplot2", "ggthemes", "directlabels", "lubridate", "car"))
libraries_needed <- c('data.table','tidyr','plyr', "dplyr", "ggplot2", "ggthemes", "directlabels", "lubridate")
lapply(libraries_needed, require, character.only = TRUE)

# disable scientific notation:
options(scipen = 999)

# Loading the data using fast data.table reader function: fread
# PATH = "/Volumes/GoogleDrive/My Drive/Academics/Capstone/Actblue"
#setwd(PATH)
PATH = getwd()
PATH
```

```{r}
### remove old objects from memory if needed:
# rm(ci, conf.intervals, dfs, dt.tenth, dt_tenth, lm_lin, lm1, lm2, new.dat) # remove by name
# rm(list=setdiff(ls(), "object_to_keep")) # remove all objects except these objects to keep
```

```{r}
# load initial dataset
df_original <- fread(paste0(PATH,"/data/dt_bernie_repeaters.csv")) # (complete or relative) path of dataset
setDT(df_original) #set as data.table object
summary(df_original)
```


# State Time Period Definition
Let's assume we are working with a dataset of actions: such as purchases, requests, donations, visits, clicks etc.
We'd want to decide on a time-period to model as one time-step that contains all the most important patterns and cycles of the data, and transform the data into a dataset where each row is that time-period and each column (i.e., amount) summarizes the total values of that variable during that time (i.e., sum of dollars spent that week, or minutes spent on the website per day).

First, we need to think and define the time-period that we'll call a "state", so that we can later chunk the data into rows that summarize these time-periods and call them steps. 
For this, we'll need to use our on reasoning about the nature of the area and problem. 
On one hand, we don't want too wide timeframes, since then we would lose valuable information hidden inside by summarizing it with just a few metrics. On the other hand, we don't want too small time-frames, since they don't reflect true cycles, lack sufficient information, and we usually won't have enough data to justify those.
We want to charachterize what are the natrual "cycle" period of the space.
For example, let's consider modeling the requests of a user of a ride-hailing platform such as Uber/Lyft/Via. Starting from smaller timeframes - naturally, hours would not be enough to capture ride-hailing patterns; a day might catch some (i.e., commute patterns or evening outings), but each day would have too much random variability such that it's not good for inference. On the other hand, we would also miss grander scheme information from skipping over the next time-frame scale: weekly patterns. We might model each week as a step, since people have their weekly routines and have different likelihoods of wanting to take Ubers both at different times of day but also at different times of the week (i.e., peaking at Friday and Saturday nights). This is probably where we should stop, since we don't have as strong monthly patterns; and while we might have yearly seasonal effects, it is too broad for us to make inference upon. If we have that data, we should verify all those assumptions in the data first. Once we decided on a data time-step, we will later reshape the data to a "wide" dataset where each row contains summary statistic (such as sum or average amount of each column in the original dataset)

#### Actblue: Monthly States (time-periods)
In this case, we are investigating patterns of donations to election campaigns through the donation platform "actblue". Through analyzing the data (that I've done previously in python here: https://github.com/tomereldor/actblue-donors/blob/master/notebooks/Tomers-actblue-preprocessing.ipynb.), I observed that the majority of repeating donations are done every month; with a few different tiers like weekly and daily donations, as well as lower frequencies or just occasional donations without a reoccuring automatic cycle.
For that reason, it seems that we should model donations per month, since if it would be shorter, we'd miss the common monthly cycles; but any longer would be too few "steps" in our data that spans for about a year (between 2015-2016). 


# Data Preprocessing
I did much of the data preprocessing already in python. You can find it here: https://github.com/tomereldor/actblue-donors/blob/master/notebooks/Tomers-actblue-preprocessing.ipynb.
You will need to inspect and clean the data, remove the irrelevant or messy variables, limit the time scope of the data if it is too long, maybe the geographical and other scopes, and make sure you are left with only the population you really want to study.
For example, I for now only care about "repeating" donors - donors who have donated more than once. For that, As you could see in the python notebook, I integrated many datasets of different time periods, and extracted only the relevent repeating donors.

Now I'll start cleaning and transforming the dataset to the desired format.

## Dataset Formatting
First, let's make sure each column has the right format
```{r}
summary(df_original)
```

There are three main types of variable formats that we care about for this case:
  1. Numeric - any numeric value (either int or float) that we will make numeric calculations with or should treat as a scale.
  2. Date - date information has its own format, which we should use to do calculations with and extract elements from.
  3. Factor - means categorical variable. 
      A related format can be "character". That would be good to capture a column of unique strings of text, such as comments or notes. However, if we want to use those for grouping or subsetting (like username), we should convert those to factors since it is really a category.
We see that V1, that should be just a numeric index, is just "V1", but it's ignorable. 
Donor_uid is of class charachter while it should be either numeric or a factor - in our case, it's a text-base ID and thus can't be numeric and must be a factor. An alternative would be to convert this ID to a numerical id and do operations on it; but in our case, it holds information (name and zip code) which is nicer to observe. 
Date should be converted into a date format, as well as previous date.
Amount, aggregate amount, number of donations, and days_since last are already in numerical format.

```{r}
# convert donor ID to a factor (categorical) rather than character:
df_original$donor_uid <- factor(df_original$donor_uid) 
# convert date columns to dates rather than text
df_original$date <- as.POSIXct(df_original$date)
df_original$prev_date <- as.POSIXct(df_original$prev_date)

head(df_original)
```

```{r}
summary(df_original)
```

## Trasnforming to timeframe summaries
Now we will need to start summarizing the DF per user per state.
I'll use data.table from now on for most such purposes.
Our State timeframe is a MONTH. Thus, first step is to add a MONTH column, so that we can group by it.
We could either choose to do monthly on a variable start day per user according to their first donation, but since we are summarizing all the month's information into one row, it doesn't really matter and we can also do the simpler way of grouping per calendar month. If someone makes a donation every 15 of the month, it will still be summarized to the same aggregate amount a month as it would if she would donate every 1st of the month.

*I'm using the "data.table" package* to serve as my data structure and main data manipulation package, since it is highly more efficient and convenient than base R. I highly recommend working with it! It is actually easy to learn and will make your R coding more efficient. Check it out here:
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html

The basic concept to understand is that data.table queries and performs operations by this syntax:
DT[i, j, by]

R:      i (rows)        ,  j (columns)      ,    by
SQL:  where | order by  , select | update   ,  group by

* However, if you are about to make dummy / categorical variable operations (like regression) with data.table, be careful with the size of the dataset, vector and memory of your machine, since such operations effectively add another column per category in the background (one-hot encoding).

Below, I'll take a sample of data and do the same transformation to it using data.table and using ddply, the common alternative, and show the efficiency of each.

```{r}
# let's take just the year + month. Since the data all lists in the same format YYYY-MM-DD, it's easiest just to take the first 6 charachters.
df_original$monthyear <- substr(df_original$date, start = 1, stop = 7)


# I always prefer to experiment first with a smaller sample of the data before doing a heavy or risky operation on the whole dataset. Let's make that sample:
dt_sample <- sample_n(df_original, 100000)
dt_sample <- setDT(dt_sample) # ensure it's a data.table
nrow(dt_sample) # checking: it does have 100000 rows. 
setkey(dt_sample, donor_uid, monthyear)


## Let's show that data.table is more efficient than the common alternative, ddply:
system.time( 
  dt <- dt_sample[ , list(total_amount = sum(amount), 
                               avg_amount = mean(amount), 
                               n_donations_month = .N,
                               n_donations_life = mean(n_donations)
                                ), 
                        by=list(donor_uid, monthyear)]
)
# first position: empty, meaning selecting all rows
# third position: by=list(donor_uid, monthyear) : here we group by donor ID and MONTH
# second position: here we specify what happens to each column 

#  user  system elapsed 
#  0.052   0.008   0.014 
```

This took 0.014 seconds using data.table

```{r}
# using ddply
system.time( 
  dt_sum_ddply <- ddply(dt_sample, 
                        ~ donor_uid + monthyear,
                        summarize, 
                         total_amount = sum(amount), 
                         avg_amount = mean(amount), 
                         n_donations_life = mean(n_donations)
                        )
  )   
#    user  system elapsed 
# 171.593   0.644 173.036 
```

Using ddply it took 3+ minutes (173 seconds) versus 0.014 seconds in data.table, meaning **data.table was 12,000 times more efficient**!
Let's do this using data.table on the entire dataset
```{r}
setDT(df_original)
dt <- df_original[ , list(total_amount = sum(amount), 
                               avg_amount = mean(amount), 
                               n_donations_month = .N,
                               n_donations_life = mean(n_donations)
                                ), 
                        by=list(donor_uid, monthyear)]
head(dt,30)
```
We will continute to use data.table whenever possible and I advise you to utilize it yourselves. 

## Time-Series States Dataset
Now that we summarized the dataset to a month-level, we will use it to create month long states. 
**We want to create a continuum of states per each donor.** Meaning, we want for each donor to have a list of what happened during EACH month of the 10 months; and if they didn't make any contribution during that month, we want that row saying that they had 0 donations!
This is in order to create a continuous representation of states per donor/user.

Let's go and fill the data for missing lines:

```{r}
setkey(dt, donor_uid, monthyear)
dt <- dt[order(donor_uid,monthyear)]
```

Now, we have many people who only donated one month. That will not add much valuable inforamtion. 
How many people donated for more than one month?
```{r}
# let's add a column: "month_donated" - in how many different months this donor donated?
# meaning, how many listings does this dataset hold of the same donor?
dt[, months_donated := .N, by=donor_uid]

# let's plot this as a histogram
qplot(data = dt, x=months_donated, main = "How many months did people donate in?", xlab = "Number of Months Users Donated In")
```

For now, for the sake of this exercise of analysis, let's work just with donors who donated in more than one month. This will mean that we'll not have a good overall picture of the probabilities of donating more than one month, but we will limit our analysis FOR THOSE WHO DONATED IN MORE THAN ONE MONTH, to really inspect the hypothesis of continuous / repeated donations versus one-time donations. 


```{r}
# SUBSET THE DATA TO DONORS WHO DONATED MORE THAN ONCE
dt_repeaters <- dt[months_donated>1, ,]
# let's work with the data subset of repeating donors:
# get a unique list of all IDs and unique list of all possible monthyears
# unique_ids <- levels(dt_repeaters$donor_uid) # the levels are based on the original full dataset. Let's do just for this one:
print(paste("Number of repeated donations:", nrow(dt_repeaters)))
unique_ids <- unique(dt_repeaters$donor_uid)
print(paste("Unique repeated donors:", length(unique_ids)))

unique_monthyears <-  sort(unique(dt_repeaters$monthyear))
print(paste("Months:", length(unique_monthyears)))
print(sort(unique_monthyears))

# Now we need to make a new dataset of all permutations
# there are a number of ways to do this.
# 1) from base R, you can use expand.grid(column_1, column_2).
# 2) from tidyR package, you can use the slighlty improved "crossing": crossing(column_1, column_2) and get a tibble.
# 3) in data.table, you can use "CJ" function, which creates a join data.table, with the "UNIQUE" option on.

# Now, let's use data.table join data table to get a data table with all permutations. 
library(data.table)
dt_combos <-  CJ(donor_uid = unique_ids, monthyear = unique_monthyears, unique = TRUE)
head(dt_combos)
```


Now that we have our base table with all required IDs and timesteps, we can fill in data where we have recorded values. The rest will be donations of 0 since they didn't donate.
Since we need ALL rows from the first (LEFT) table of permutations and keeping even the ones without matches from the RIGHT table, while inserting only the matched rows from the second (RIGHT) table, this is called a LEFT-OUTER JOIN - a concept taken mainly from SQL language. It is implemented in multiple ways in R: as merge functions in base, in dplyr, and in our case in data.table. For a more complete guide on joins in data.table, see this tutorial:
https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html
```{r}
# now, combine this permutations dataset with the actual info from our dt_repeaters dataset.
# first, we need to set the columns we want to match on as keys:
setkey(dt_combos, donor_uid, monthyear)
setkey(dt_repeaters, donor_uid, monthyear)

# Now, we perform the match!
# The "all.x" part is the "Left Join", meaning: 
# take everything from the LEFT table and match what you can from the RIGHT (X=LEFT, versus RIGHT=Y)
dt.all <- merge(dt_combos, dt_repeaters,  all.x=TRUE)
dt.all
```

GREAT! Now we have the merged data.table. 
BUT WAIT - there's NAs! Why is that? of course, we just matched the rows with donations, while leaving the rows without donations as NA's.
Now all we need to complete this stage is to fill out the dataset NAs with the relevant values: 0's for the amounts and n_donations, and the unique user value for n_donations_life and months_donated.

```{r}
# let's fill out the NA lines with relevant values:
dt.all[is.na(total_amount)]$total_amount <- 0
dt.all[is.na(avg_amount)]$avg_amount <- 0
dt.all[is.na(n_donations_month)]$n_donations_month <- 0
dt.all$monthyear <- factor(dt.all$monthyear)
# now fill n_donations_life and months_donated with the constant corresponding statistic per user
# first, get a condensed table of unique matches: total n_donations_life and months_donated of each user (regardless of month). This is like a dictionary we'll later multiply
dt.totalstats = unique(dt.all[!is.na(n_donations_life), .(donor_uid, n_donations_life, months_donated)])
setkey(dt.totalstats, donor_uid)
setkey(dt.all, donor_uid)
dt.all <- merge(dt.all[,.(donor_uid, monthyear, total_amount, avg_amount, n_donations_month)], dt.totalstats, all.x = TRUE)

#save file. "fwrite" is a quicker and efficient writer of data.tables
fwrite(dt.all, paste0(PATH,"/data/dt_all.csv"))

print(summary(dt.all))
head(dt.all)
```


```{r}
## loading checkpoint if needded:
# dt.all <- fread(paste0(PATH,"/data/"))
```


# Defining States - Data Exploration and Analysis

In order to define our states, it is important to know our problem space well, the analyzed users relatively well, and what is the purpose and usage of this analysis (who is going to use it and for what).
There are two directions we can take this in:

## 1. Manual Definition from Human Knowledge

*1. User:* Who is using this and for what? Do they already have groupings of users that we should consider? Do they treat customers differently based on some criteria or thresholds? 
*2. Product:* Does our product or service offer distinct tiers? If so, we shuold consider those as differences in states. If not explicitly, does it do so implicitly?
*3. User Journey:* The best state maps would map the life journey of the user with our product / service. For example: new -> first_time_user -> weekly_high_amount_user -> monthly_high_amount_user -> churned.
By knowing the usual journey flows of our customers we'll be able to construct users stories and thus build some hypotheses for possible stages of a user's life. If we don't know that already, we can learn that by looking at the data. 

## 2. Computational Algoithms to help define states
We can use some algorithms to help us define the states here!
Two main ways we can do that are as follows:

*1. Clustering into states:* decide which few features (columns) of the data are most important at defining a state? For example, frequency and amount. We can choose those and cluster the data according to those. If we are lucky enough to be able to cluster according to only 2 (or sometimes 3) features, we can also plot all the data's point values on a scatterplot with these two features, and plot the resulted state clusters or boundaries to help discern the quality of clustering.

*2. Predicting next states* - if you use a regression or better yet a decision tree or random forest on all the features to predict an outcome such as the next state, or the next amount donated next month, you could find what are the most valuable data-pieces!

*3. Hidden Markov Models* - Hidden Markov Models assume that there are some hidden processes which drive our observed processes above the ground. We can use some visible variable such as the "amount" as the measured proxy (visible state), while assuming that there are hidden states which drive it 
invisibly in the background. 
This approach should also maximize the stability of those states, although potentially at the expense of the interpretability of the states themselves. 

## 3. Combine computational data analysis with domain knowledge to creaete states
if you don't have your states of engagement already, you should think on what are useful and reasonable "states" of engagement for your case. This process shouldn't really be entirely automatic, because eventually this sort of analysis needs to have understanble states that would hopefully be relevant to (1) understand and talk about your customer base more broadly in other uses and (2) treat these "states" as cohort and make decisions accordingly, such as treating or incentivizing them differently, or such as inferring customer behavior patterns from the transitions between them.
However, we do want the states to be rather stable, meaning - the probability of staying in each state should be relatively high (except for states defined differently, such as if you define a "new" state or another inherently temporary state).
With that, devizing the states should be informed by understanding data and customer behavior. 
For inspiration, see below. 

Suggested Procedure for discerning states:
1. Decide what is the metric for "value" that you want to measure. In many cases that would be money, but not necessarily, such as time spent on your platform, clicks, or whatever drives your revenue or key business outcomes
2. Decide what are the most important variables to differentiate between states. In many cases those will be: (1) frequency per time period (averaged) and (2) value delivered. 
3. Think organizationally and conceptually: do you have organizational relevance to particular types of divisions across these variables? for example, frequency of every order of magnitude (amountly, daily, weekly, monthly, yearly), or in value levels? (are there different tiers of customers according to their value delievered? such as "gold" status in a frequent flyer club; are there currently any promotions or segmentation of the customer base that is likely to persist and could inform these states here?)
4. Explore the distribution of these two variables. Does the data lend itself to some distinct groupings? Do these make sense?
5. Try to combine the logical and data-driven hypotheses about states and inform your state formation.
6. Alternatively, or in combination to inform the above, you could let the data speak for itself - and use a clustering algorithm to automatically choose optimal clusters along the measured variables. I'll demonstrate such a technique using a simple and famous clustering algorithm called K-means.
7. Another possible direction, which would be usually mainly useful later on in the process, is to use Hidden Markov Models (HMMs) - which assumes that there is a visible layer of states that is driven by an unkown, hidden layer of states. However, we need to start off with some "state" definitions as the visible states. 


## Clustering Approach
If you have very specific domain knowledge and business use-case describing your states already, that's great, and that's something I can't help with in generalizing. Therefore, I'm going to to take a  more domain agnostic and generalizable approach here and create first definition of states being informed by clustering.

In our case, there are 2 important variables which would define a state: 
1. Amount donated, and 2. Frequency.
Amount donated - we have that information already, as a continuous amount donated in aggregate over the month. We currently have two amount metrics: average amount donated that month, or total amount donated that month. Which one will we choose depends on what is the other variable of choice, and which would maximize information given combined with the other variable.
Frequency - we currently have number of donations this month. We might want to include more past information; for example - how many consecutive months have you donated? That might be more predictive. 
However, the Markovian assumption is that the states are "memoryless": the current state is supposed to be sufficiently predicitve about the next ones. 
Therefore, we will start with a 1-step markov decision process. Later we can try incorporating more memory by wrangling the data to have a more memory-based metric, or by implementing a multi-step markov model.

Our variables of choice then are:
For frequency, the existing 1-step variable is N_Donations per month. For amount, we have either average amount per donation, or total amount donated that month. Considering that we already have the number of donations as another variable, it makes more since to use the Average amount donated, which gives us a more intuitive metric of the actual average amount value (i.e., $1 per day), and we can easily multiply it by the N_donations per month to get the monthly total if we like.

First, we will explore the data distributions around these variables of interest:

*Distribution of N_Donations Per Month*
```{r}
qplot(x = n_donations_month, binwidth=1, data=dt[n_donations_month>0], xlab = "Number of repeated donations", main = "Histogram: Number of repeated donations per donor") 
```
So the number of repeated donations per person turns out to be logarithmic with exponential decay.

*Distribution of Average Amount Donated*
```{r}
qplot(x = avg_amount, binwidth=5, data=dt[n_donations_month>0 & avg_amount<=500], xlab = "Average Amount (for months with donation)", main = "Histogram: Average Amount Donated") 
```
We can see a mostly poisson like distribution, but it's mainly multimodel comprised of commmon sums to donate. This is probably *because the Bernie Sanders ActBlue website campaign encouraged candidates to  buttons of suggested donation values* of these peaked values:
- $10
- $27
- $50
- $100
- $250
- $500
- $1,000
Or: Other [Insert Value]. 

This is a clear example of how *NUDGING* and user architecture really influences users to follow nudges; the given donation amounts were almost all of the donations.


#### Scatterplot of relationship between main two variables
if you want to be agnostic to the sizing of each feature, you should normalize / standartize all of your features before clustering. In this case I want that proportion of weighting, so I'm keeping the sizes as is for now.

Let's see if there are any visible divisions of the data:
#### Scatterplot of avg_donation_amount vs n_donations_month
```{r}
# let's see the relationship between donation_amount vs days_since_last_donation
# we'll sample 500,000 out of the 2.1 Million rows with actual donations we have, since R couldn't produce that image with all those datapoints!
qplot(x=n_donations_month, y=avg_amount, data = sample_n(dt.all[n_donations_month>0],500000), geom="point", main="Monthly donations vs Donation Amount", xlab = "Number of donations that Month", ylab = "Donation Amount", alpha=0.1)
```
These are the main two axis upon which we will create our states, since we are interested in groups of varying (1) frequency and (2) amount. Therefore, plotting these variables against each other is important. In some cases, the data might have had more natural divisions by itself. In other cases, you might want to transform the data in some ways to have more granularity or a different measure of a similar metric so that it would be more easily divisible (for example: number of donations in the past 2 months; days since last donation, etc).


### Get States Approximations Using Clustering


## Grouping #1: States by 1 month (completely memoryless)
if you want to be agnostic to the sizing of each feature, you should normalize / standartize all of your features before clustering. In this case I want that proportion of weighting, so I'm keeping it like this.
The two most important variables for us for clustering are: average donation amount vs number of donations per month. 
We might want to check later other transformations of variables which might be more accurate, but it depends on our later application, like: the number of consecutive months with donations, or that weighted by the number of donations per month. 

Let's see what clustering might suggest for us.
We will try to cluster the data according to our 2 (or more) most important variables - usually reltaed to frequency (here: days since last donation) and amount (could be also # of amounts / time spent / clicks / egnagement etc). 
```{r}
# Make sure to clean your values from NAs and INFs, and if you want - trasnform them into making more relevant groupings (for example, raising to the second power if you want more distinct differences at higher amounts).
summary(dt.all)
```

There are many clustering algorithms available. For this tutorial purpose, let's use the k-means, since it is the most common one, most familiar to readers, easily understable, and its algorithm answers our needs. We'll use one of the common algorithm to each k-means clustering called "Hartigan-Wong".
The Hartigan-Wong updates its centroids every time when a point moves. Additionally, it also saves time by making smarter choices in how it checks for the closest cluster. Other available algorithms in R, like Lloyd's k-means algorithm, are not as efficient, but rather remained as historical legacy since Lloyd's was the first (and thus least developed) clustering algorithm.

```{r}
# now let's see what clustering would bring us
cluster <- kmeans(x=dt.all[, .(avg_amount, n_donations_month)], centers=6, iter.max = 500, nstart = 25, algorithm = "Hartigan-Wong")
cluster
```

Now after we algorithmically clustered, let's see where did the algorithm divide the data:
```{r}
# See results

## Now we got states and interpretations for them. let's insert them into our data.
dt.all$cluster <- factor(cluster$cluster) # adds a cluster column to inform states 

# now we can plot this and see how it looks. I'm sampling 0.5 Million samples from the data so that the software can handle it. For plotting, I'm plotting average donations below 1000 since above that there are only some outliers (that belong to the same group).
ggplot(sample_n(dt.all[avg_amount<1000], 500000), aes(x=n_donations_month, y=avg_amount, color=cluster, alpha=0.2)) +
  geom_point() +
  labs(title =  "Avg Amount vs #N Donations This Month", 
        x = "Number of Donations This Month", 
        y = "Avg Donation Amount")

```
The clustering algorithm clustered the data into 1- different clusters, where we see the following trends:
1. They are mostly clustered by amount and not frequency
2. There are many more groups in the lower range; almost as if it was a more linearly spaced division in the log-scale. 
We shall consdier that but also other alternatives in choosing states.


#### Choice of the clusters themselves
As I mentioned before, the choice of states should be informed by data but more importantly - for our needs of later interpretation. 
The clustering algorithm chose to cluster mostly according to the amount donated, but we care maybe even more about the number of donations per month. 
Looking at the clusters, the data, previous data explorations and distributions, and the possible choice set for donations, we should come up with some distinct boundaries.

#### Choice of the number of clusters
How do you know how many clusters to have? This is really a case-by-case question. If you have some definitions and cut-offs that would make sense to cluster by, defnitely consider those. You probably don't want too many clusters because you want them to be interpretable and understandable. My advice is go with a few clusters which are clear and interpretable in your situation, and mix insights from clustering methods, possibly Hidden Markov Models (see end of tutorial), and real-world relevance of groups.
Computationally, I'd look at the distribution of clusters and see what looks most sensible (the most concise distinct clustering framework), while keeping an eye for the reported Cluster Sum of Squares by cluster (between_SS / total_SS =  87.8 %). 
After many trials and various amounts for the centers, this latest one included above seems to have good performance and decent interpretability. We'll continue with that for now.

#### Check clusters boundaries to inform states boundaries, together with external information (of choice set)

Check clusters means on both axes
```{r}
cluster$centers
```


Check cluster boundaries

```{r}
## borders in amount
list_borders <- c()
for (i in seq(1,10)) {
  list_borders <- c(list_borders, c(min(dt.all[cluster == i, avg_amount]), 
                                  max(dt.all[cluster == i, avg_amount])))
}
list_borders
```

```{r}
## borders in n_donations_month
list_borders_nd <- c()
for (i in seq(1,10)) {
  list_borders_nd <- c(list_borders_nd, c(min(dt.all[cluster == i, n_donations_month]), 
                                  max(dt.all[cluster == i, n_donations_month])))
}
list_borders_nd
```



For determining boundaries, we should take again a close look on the histogram:

```{r}
qplot(x = n_donations_month, binwidth=1, data=dt.all, xlab = "Number of repeated donations", main = "Histogram: Number of repeated donations per donor") 
```

```{r}
qplot(x = avg_amount, binwidth=1, data=dt[n_donations_month>0 & avg_amount<=100], xlab = "Average Amount (for months with donation)", main = "Histogram: Average Amount Donated") 

```
In addition to the histogram, we should consider the choices given to donors were:
`$10, $27, $50, $100, $250, $500, $1000, [custom_value].`
But we mainly see the peaks (outside of 0) at: `$10, $25, $50, and $100`, with very few donations above a $100. 

According to the above means and boundarie, previous data explorations and distributions, and the possible choice set for donations, I will be clustering according to the following:
*Amounts donated*
0: non-donated
0< x_between ≤$10, - "1"
10< x_between ≤$25, - "10"
25< x_between ≤$50 - "25"
50 + and higher  - "50" 

*Number of donations per month:*
- 0 donations: non-donated
- 1-4 donations - "monthly"
- 4-10 donations - "weekly"
- 10+ donations - "daily"


#### Creating states according to boundaries
```{r}
#f_divide_states <- function(dt) {
dt.all$state <- ""

# No donations:
dt.all[n_donations_month == 0]$state <- "Non" #0
dt.all[avg_amount == 0]$state <- "Non"

# low freq (Monthly): 1-4 donations (monthly to weekly donations)
# for each tier, we have 4 pricing tiers: 
# $0.1 - $10, $10 - $100, $100 - $500, $500+
dt.all[n_donations_month>0 & n_donations_month<=4 &
        avg_amount > 0 & avg_amount<=10]$state <- "monthly1"    #31
dt.all[n_donations_month>0 & n_donations_month<=4 &
        avg_amount > 10 & avg_amount<=25]$state <- "monthly10"  #32
dt.all[n_donations_month>0 & n_donations_month<=4 &
        avg_amount > 25 & avg_amount<=50]$state <- "monthly25"  #33
dt.all[n_donations_month>0 & n_donations_month<=4 &
        avg_amount > 50]$state <- "monthly50+"                  #34

# weekly: 4-10 donations
dt.all[n_donations_month>4 & n_donations_month<=10 & 
        avg_amount > 0 & avg_amount<=10]$state <- "weekly1"  #21
dt.all[n_donations_month>4 & n_donations_month<=10 &
        avg_amount > 10 & avg_amount<=25]$state <- "weekly10" #22
dt.all[n_donations_month>4 & n_donations_month<=10 &
        avg_amount > 25 & avg_amount<=50]$state <- "weekly25" #23
dt.all[n_donations_month>4 & n_donations_month<=10 &
        avg_amount > 50]$state <- "weekly50+"               #24


# daily: 10+ donations
dt.all[n_donations_month>10 &
        avg_amount > 0 & avg_amount<=10]$state <- "daily1"  # 11
dt.all[n_donations_month>10 &
        avg_amount > 10 & avg_amount<=25]$state <- "daily10" #12
dt.all[n_donations_month>10 &
        avg_amount > 25 & avg_amount<=50]$state <- "daily25" #13
dt.all[n_donations_month>10 &
        avg_amount > 50]$state <- "daily50+"                 #14

dt.all$state <- factor(dt.all$state)  
fwrite(dt.all, paste0(PATH,"/data/dt_state.csv"))
summary(dt.all$state)

```


```{r}
# view means and sums of donations by states

x[ , c("mean", "sum") := list(mean(b), sum(b)), by = a][]
dt.all$state_num <- 0
dt.tenth[n_donations_month>10 &
        avg_amount > 0 & avg_amount<=10, c("state", "state_num") := list("daily1",  11)][]

```


```{r}
#install.packages("car")
#library(car)
dt.all$state_num <- recode(dt.all$state, 
    " 'Non' =       0 ; 
    'monthly1' =    31 ;
    'monthly10' =   32;
    'monthly25' =   33;
    'monthly50' =   34;
    'weekly1'       21;
    'weekly10' =    22;
    'weekly25' =    23;
    'weekly50' =    24;
    'daily1'  =     11;
    'daily10' =     12;
    'daily25' =     13;
    'daily50+'      14; ")
```



##### checkpoint to reload data from disk:
```{r}
### load data from disk
dt.all <- fread(paste0(PATH,"/data/dt_state.csv"))
dt.all$state <- factor(dt.all$state)
summary(dt.all$state)
```


Here we finished setting up the states to a logical division, informed by the distribution of the data, somewhat by clustering analysis (although that was deemed less relevant in this particular case), and created states of donors which are graspable, discriminable, easy to understand, and revelant for this analysis.

## Visualize the new states

```{r}
ggplot(sample_n(dt.all[avg_amount<1000], 500000), aes(x=n_donations_month, y=avg_amount, color=state, alpha=0.2)) +
  geom_point() +
  labs(title =  "Avg Amount vs #N Donations This Month", 
        x = "Number of Donations This Month", 
        y = "Avg Donation Amount")
  

```
I now want to plot this in a slightly more visualization thoughtful way - more easy and intuitive to read, less distractions from the actual data - like white background and better sizes of visualizations, more appropriate for paper-ready graphs and make data more easily detected by the human eye. This is based on visualization principles, and you can learn more about them by the field expert Claus O. Wilke, here: https://serialmentor.com/dataviz/. Wilke created the "cowplot" package which automatically transforms your ggplot visualizations to be of that style. It's not necessarily more visually pleasing, but it is better for our human detection and understanding of the data in the plot more effectively.
```{r}
# install if necessary and load the COW package, then plot normally!
#install.packages("cowplot")
library(cowplot)
```

```{r}
ggplot(sample_n(dt.all[avg_amount<1000], 750000), aes(x=n_donations_month, y=avg_amount, color=state, alpha=0.1)) +
  geom_point() +
  labs(title =  "Avg Amount vs #N Donations This Month", 
        x = "Number of Donations This Month", 
        y = "Avg Donation Amount")

```



# Create "Previous State" shifted column for transition
Now that we have created the "state" column, we need to create another column for each row of the "previous state" for each agent. From this, we'll be able to build the transition matrix.
For this operation, I'll again use data.table. 
In order to create a shifted state column, we first need to order the dataset in the order to be shifted, and group by user (so that states don't spillover between users). So the primary sort column is user, and then we want to order that by date.

## Dealing with Big Data
This was too large for a single machine to compute. The dataset has 6 Million entries.
How can we deal with that?
One way to go around it could be chunking:
1. Divide the dataset to K seperate chunks
2. Perform the desired operation on each chunk K seperately
3. Save the result on disk, seperately
4. (Idally, remove all unnecessary objects from memory, including this last calculated dataset)
5. Repeat these steps for each K chunk
6. Merge all transformed datasets.
However, when you perform operations on smaller chunks of the dataset, you must make sure that the *splitting* wouldn't pose any problem.
For example, here we are shifting a column based on subgroups by donor. Therefore, we must make sure that our chunked dataset will end with a complete listing of all donations of a donor and not cut in its middle. 
How can we do that? 
I generate a list of unique donors, and I divide *that list* to 10 (or K) ~equal chunks. 
Then, I create a temporary dataset each time from the subset of the whole dataframe of specifically these donors.
Thus, I ensure that each donor has a complete representation in their chunk and no listings are incomplete or splitted between chunks. 
```{r}
nrow(dt.all)
```

```{r}
# create a list of unique donor IDs
unique_ids <- unique(dt.all$donor_uid)
# what's the length of a 1/10 of that list?
tenth <- round(length(unique_ids)/100)

```

Now we start the splitting
```{r}
# get tenth of the ids 
tenth_ids <- unique_ids[1:tenth]
length(tenth_ids)
dt.all$state <- factor(dt.all$state)
dt.tenth <- dt.all[donor_uid %in% tenth_ids]
```


```{r}
# now for each division of the db perform the shift
dt.tenth$state <- factor(dt.tenth)
# First step: "Set keys"" in data.table by the order we want to sort by the columns. make sure the first column is the primary group (the user: donor_uid) and the second is the secondary (dates: monthyear)

setkey(dt.tenth, donor_uid, monthyear) # important for ordering 
dt.tenth <- dt.tenth[order(donor_uid, monthyear)] 
# verify it's ordered correctly by viewing it:
head(dt.tenth,30)
#nrow(dt.tenth)
# CREATE "Previous State" COLUMN, SHIFTED ONCE BEHIND, per group of user id. 
dt.tenth[, prev_state:= (shift(state, n=1L, fill = 0 , type = "lag")), by=donor_uid]

# save result
fwrite(dt_tenth, paste0(PATH,"/data/df_chunks/dt_states_",1,".csv"))

# make sure it worked!! see below for further description of what to notice:
dt.tenth[1:100, .(donor_uid, monthyear, state, prev_state)]
```

Then you do this for every piece of the dataset and combine the restuls; or do this all on a strong enough machine.

or altenratively, do the above for all at once if you have a strong enough machine:
I've commented these out for not accidentally running them again.
```{r}
# now for each division of the db perform the shift
# First step: "Set keys"" in data.table by the order we want to sort by the columns. make sure the first column is the primary group (the user: donor_uid) and the second is the secondary (dates: monthyear)
# 
# setkey(dt, donor_uid, monthyear) # important for ordering 
# dt <- dt[order(donor_uid, monthyear)] 
# # verify it's ordered correctly by viewing it:
# head(dt,30)
# #nrow(dt)
# # CREATE "Previous State" COLUMN, SHIFTED ONCE BEHIND, per group of user id. 
# dt[, prev_state:= (shift(state, n=1L, fill = 0 , type = "lag")), by=donor_uid]
# 
# # save result
# fwrite(dt_tenth, paste0(PATH,"/data/df_chunks/dt_states_",1,".csv"))
# 
# # make sure it worked!! see below for further description of what to notice:
# dt[1:100, .(donor_uid, monthyear, state, prev_state)]

# create "New" state for new donors who don't have a previous state, and are not in the first month where everyone doesn't have a previous step.
# dt[isnull(prev_state) & monthyear != '2015-04', state:="New"]
```



Checkpoint to save or load the saved result:
```{r}
# fwrite(dt.all, "./data/df_states_full.csv")
```

```{r}
dt <- fread("./data/df_states_full.csv")
dt$state <- factor(dt$state)
print(summary(dt$state))
head(dt)
```


### Filter First Month (without transitions)
For the upcoming calculations of states, we want to ignore the first month because no one has a previous state there. So we filter those out:

```{r}
#dt_backup <- dt # backup dt
dt <- dt[dt$date > as.Date("2015-04-02"),]
df <- setDF(dt) # make a data.frame out of dt 
```




# BUILD TRANSITION MATRIX 
Now we can use our dataset and build a Markovian Transition Matrix based on the frequency of current transitions!
## Build Overall Transition Matrix: Based On All Data
```{r}
# First, make sure your states are ordered in your desired order.
# in R, cateogircal columns would be "factors", and you need to do that by *specfying the order of the "levels"* in your factor. 

state_levels <- c("New", 'Non'  , 'monthly1' , 'monthly10', 'monthly25', 'monthly50+', 'weekly1' , 'weekly10' , 'weekly25' , 'weekly50+' , 'daily1', 'daily10', 'daily25', 'daily50+')
dt$state <- factor(dt$state, levels = state_levels)
dt$state_prev <- factor(dt$state_prev, levels = state_levels)

# This is a function which computes Transition Matrix from data and converts to vector 
f_compute_transitions_as_matrix <- function(df=dt, 
                                          curr_state_column = "state", 
                                           prev_state_column = "state_prev",
                                          return_ns=F, #the return Ns will return also the N amount of transitions
                                          factorize=F) { 
    # sometimes we want to make sure the columns are factored correctly, that could be handles here:
   if (factorize) {
      # transform the data into a contingency table of counts of joint occurences (previous & current states)
      dt$state <- factor(dt$state, levels = state_levels)
      dt$state_prev <- factor(dt$state_prev, levels = state_levels) }
  
    # melt the pairs of transition from wide format into long format
    transitions_long <- melt(table(df[[prev_state_column]], df[[curr_state_column]]))
    colnames(transitions_long) = c("state_prev", "curr_state","n") # change column names 
    
    # here we create the transition matrix by summarizing the long format, where the key is the current state
    trans_matrix <- spread(as.data.frame(transitions_long), 
                           key =  curr_state, value = n, fill = 0,  drop = FALSE) 
    trans_matrix[["state_prev"]] <- NULL # delete the named row
    
    # Tabulate to get numbers of each row (from each state), 
    # and divide that row by this total number to get percentage of transitions
    n_per_row <- rowSums(trans_matrix) 
    trans_matrix <- trans_matrix / n_per_row # make into transition probabilities (every row sums to 1)
    trans_matrix <- replace(trans_matrix, is.na(trans_matrix), 0) # replace NA with 0 (never occured)
    trans_matrix <- data.matrix(trans_matrix) # convert into data (numerical) matrix
    #trans_matrix <- replace(trans_matrix, is.na(trans_matrix), 0) # replace NA with 0 (never occured)
    if (return_ns == T) {return(list(trans_matrix, n_per_row))} else {return(trans_matrix)}
}

```

Above this is the function that we can use to create a transition matrix out of any two columns, or subset of the dataframe.
Let's use it over the whole dataframe:

```{r}
## TRANSITION MATRIX: convert vector to transition matrix
trans_matrix <- f_compute_transitions_as_matrix(dt, curr_state_column = "state", 
                                           prev_state_column = "state_prev", return_ns = F)

# Save matrix
fwrite(as.data.frame(trans_matrix, row.names = unlist(state_levels)), "./data/trans_matrix_all.csv")

# PRINT MATRIX
round(as.data.frame(trans_matrix, row.names = unlist(state_levels)),2)

```


Great! The above transition matrix shows (as you could see more clearly colored in the report) that the *diagonial values*, meaning the *stable transitions of staying in the same state*, are the highest ones. That means that our categorization per states was reasonable, and we may continue. Naturally, it is not ideal - they are not as stable as being around 80% stable staying at the same state, but at least they are more likely to repeat than to transition to any other state (except for the state of "New" which is how I defined it). 


## Divide into "Train" and "Test" datasets
In Machine Learning, when we want to develop predictive models, we have to balance fitting our model best but not overfitting. We test this by dividing our dataset into a "training" and "testing" datasets: we fit our model based on the training dataset, and then finally we test it and report conclusions on the test dataset. That shows the result of generalizing this model to out-of-sample new data, which is what we wanted when we developed a predictive model (otherwise, we would have known the answers already, no?!).
Normally, we'd want our training and testing datasets to be similar in terms of being representative random samples from the population.
However, in this case, such as in any other time-series or chronological dataset, in reality we will always have a history of data and will try to predict for the next unseen future data based on the past data. Therefore, we will actuallly need to divide our training and testing datasets by chronological date to early and late data, treating it as if we are now at the midpoint in time, and based on all that past data we will try to predict that "future" data.

```{r}
head(dt)
dt$monthyear <- factor(dt$monthyear)
# let's convert these to dates, since we will need that also later on for plotting:
# since we left out the day of month, we now need to manually insert a random day of month to have R accept that it's a date.
dt[, date:= as.Date( paste0(monthyear, "-01") )]
head(dt$date)
# now split the dataset
dt_2015 <- dt[date < as.Date("2016-01-01"), ]
dt_2016 <- dt[date >= as.Date("2016-01-01"),]
print(paste("2015 Dataset N =", nrow(dt_2015)))
summary(dt_2015[, .(monthyear)])
print(paste("2016 Dataset N =", nrow(dt_2016)))
summary(dt_2016[, .(date)])
print(paste0("Allocated into training set: ",100*nrow(dt_2015)/nrow(dt),"%."))
```



## Build "Train" Transition Matrix: Based Only on Early/Training Dataset
Now we would also want to test the predictive capability of this transition matrix by splitting the dataset to training (until Dec 2015) and test (Jan 2016 onwards). We will build the transition matrix on the training set and predict on the test set later.
In this particular case, we will probably encounter a seasonality issue, since we don't have enough years to model to account for seasonality, and I don't presume to know enough how to predict to manually insert modifications to predictions in order to adjust for seasonality. The major change would be significant increases in donations as we get closer to the elections seasons. However, I will demonstrate this principle for the reader's potential use. 
```{r}
### 2015 Only TRANSITION MATRIX + N's of rows
list_trans_matrix_2015 <- f_compute_transitions_as_matrix(dt_2015, curr_state_column = "state",
                                                          prev_state_column = "state_prev", return_ns = T)

trans_matrix_2015 <- list_trans_matrix_2015[[1]] 
trans_matrix_2015_Ns <- list_trans_matrix_2015[[2]] 

# PRINT MATRIX
round(as.data.frame(trans_matrix_2015, row.names = unlist(state_levels)),2)
```

Now we have the transition matrix based on the data of 2015! It seems reasonable as it is fairly close to the full one. 


# BUILD MONTHLY HISTORY OF TRANSITION MATRICES
In order to substantiate the stability and over-time relevance of our model, we'd like to create the same transition matrix model based only on each month data, and later compare the values across months, so that we have the history and satilibity of transitions accroding to each month. If values change wildly (either wild oscilations or worse - a clear trend), we'd know that either (1) the monthly data is too sparse or that (2) transition patterns inherently change throughout time, and thus we might not be able to rely on the model to predict far into the future.

I'll do that by:
1. Creating a new column "month" of year-month
2. Group the dataset by "Month", and run the same trasition_matrix creation funcion on each group (using ddply that's simple)

Building Per Month Datasets:
We'll use ddply and dplyr, which work with Data.Frames and not with Data.Tables, so we'll convert the data.table into a dataframe simply by using "setDF()" 

Let's start with our monthly transition matrices and values. 
First, we'll build the same concept as before - our Markovian transition matrix, but based on each Month's data seperately. 

## Monthly Transition Matrices
```{r}
# ALL monthly TRANSITION MATRICES
f_generate_monthly_trans_matrices <- function(df, states_colname="state",
                                               prev_states_colname="state_prev") {
  monthly_trans_matrices <- dlply(df, .(monthyear), .fun = f_compute_transitions_as_matrix, curr_state_column = states_colname, prev_state_column = prev_states_colname)
  monthly_trans_matrices <- lapply(monthly_trans_matrices[2:length(monthly_trans_matrices)], na.exclude) 
  monthly_trans_matrices <- monthly_trans_matrices[2:(length(monthly_trans_matrices)-1)]
  monthly_trans_matrices
  }

# Run Function!
monthly_trans_matrices <- f_generate_monthly_trans_matrices(df)
# Save Result
fwrite(monthly_trans_matrices, "./data/monthly_trans_matrices.csv")
# View Result
monthly_trans_matrices[5:6]
```

Above is an example of the first two transition matrices based on each month.
We see, for example, that the "rare" columns aren't always filled out, since it is rare to find those rare donations (and 1 month data isn't always sufficient). That's a weekness inherent to this check, so we shouldn't count on the accuracy of the more "rare" values.

## Average Monthly Amount Per State
Since we'd want to estimate what is the 'value' of being in each state, the key information for that is what was the actual amount donated in each state per donation-timestep. 

```{r}

# ALL MONTHLY AMOUNT FLOWS (Donor Monthly Amount Averages), PER DONOR STATE, PER MONTH
f_generate_monthly_amount_avgs <- function(df = dt, 
                                          states_column = "state", timeframe = "monthyear") {
    monthly_amount_avgs <- ddply(df[c(timeframe, states_column ,"avg_amount")], 
                              .(monthyear, eval(parse(text=states_column))), 
                              summarise, avg_amount=mean(avg_amount))
    colnames(monthly_amount_avgs) = c(timeframe,"state","avg_amount")
    return(monthly_amount_avgs)
}

# Run Function: Generate Average Monthly Amounts
monthly_amount_avgs <- f_generate_monthly_amount_avgs(df)
# Save and View Result
fwrite(monthly_amount_avgs, "./data/monthly_amount_avgs.csv")
monthly_amount_avgs[1:25,]
```
Above is the average "value" (amount donated) per state per time period.


## Grab Diagonal Transition Probabilities (Probabilities of repeating the same state)
To get a sussinct idea of the differences between transition probabilities through history, I'll use the diagonals of the transition matrix, which means the probability for each state to repeat itself. Then, we could visualize their progress over time. 
```{r}
# Monthly diagnoals df
f_grab_monthly_diagnoals <- function(monthly_trans_matrices, state_levels, timeframe="monthyear") {
  monthly_diagonals <- ldply(monthly_trans_matrices, .fun = diag)
  colnames(monthly_diagonals) <- c(timeframe, unlist(state_levels))
  return(monthly_diagonals)
}
# Run function
monthly_diagonals <- f_grab_monthly_diagnoals(monthly_trans_matrices, states, timeframe="monthyear")
# View result
monthly_diagonals
```



### Monthly Transitions Of Specific States
Additionally to the diagonals, i'd like to check some transitions To or From speicifc states more closely.
For example, if our the state that donates the most throughout time is "Daily50+", we'd like to know who / how people transition into that state? and where do they go afterwards? and how has that changed throughout history?

```{r}
length(state_levels)
# monthly Row or Column: To and From Daily
f_grab_trans_fromto <- function(monthly_trans_matrices, rowcolnum = 3, from_or_to = "from", states_list = state_levels, rows_id="monthyear") {
  ifelse(test = (from_or_to == "from"),
         yes =  grab_trans <- function(df) {df[rowcolnum,]},
         no =  grab_trans <- function(df) {df[,rowcolnum]}
  )
  monthly_trans_rows = ldply(monthly_trans_matrices, .fun = grab_trans,  .id = rows_id)
  colnames(monthly_trans_rows) <- c(rows_id,unlist(states_list))
  return(monthly_trans_rows)
}
# Run Function, for example from state daily low, since that's interesting
monthly_trans_from_dailylow <- f_grab_trans_fromto(monthly_trans_matrices, rowcolnum = 11, from_or_to = "from") 
# View result
monthly_trans_from_dailylow
```




# Markov Chain
Now we can model this as a markov chain to get what would the steady state look like with the following transitions. 
```{r}
#install.packages("markovchain")
require("markovchain")

### Markov Chain
markov <- new("markovchain", states = unlist(state_levels), byrow = T, transitionMatrix = trans_matrix, name = "Driver States Markov Chain")
# print(markov)

#### Steady State
# The steady state is the EigenVector of the transition Matrix; it's the proportions of drivers in each state which will remain the same with the transition matrix.
steady <-  steadyStates(markov)
typeof(steady)


# reorder data.table
order_by_state <- function(vector = steady, new_order=state_levels) {
  vector_dt <- as.data.frame(t(vector))
  vector_dt$state<- rownames(vector_dt)
  vector_dt$state <- factor(vector_dt$state, levels = new_order)
  vector_dt[order(vector_dt$state),]
  vec <- as.double(vector_dt$V1)
  names(vec) <- new_order
  return(vec)
  }

steady <- order_by_state(steady, state_levels)
print(t(t(steady)))
```



# CUSTOMER LIFETIME VALUE EQUATION MODEL

* Modeling value as amounts of driver 
(* consider that we pay drivers differently depending on their state, so that should factor in the equation)


Our Driver Value Equation:
X = monthly_amounts + discount_factor*EXPECTED_VALUE
where EXPECTED_VALUE is: P_transition_matrix*X (times expected value next week assuming it's the same)
So:
X = monthly_amounts + discount_factor*(P_transition_matrix\*X)

We need to get this in the form of MATRIX*X=constant. So:
X = monthly_amounts + discount_factor*(P_transition_matrix\*X)
recall them in easier names:
X = h + dP*X
X - dP*X = h
(I-dP)X = h

We can here solve equation of the form MATRIX*X_VECTOR = constant  (Ax=b)
So we'll create the "A" matrix as: A = (I-dp)

#### Discount Factor: We need to manually determine a discount factor
discount = 0.8 # manually determined discount factor!
You will manually determine the discount factor and there is no exact science about this. 
One useful measure is to check: how many steps in the future do you need to know about? If you say only 4 steps, you can set the discount factor so that after more than 4 steps the discounted value is negligible, according to your "negligible" threshold. Let's say the "negligible" value is 0.05, or 5%, since this is commonly accepted as a significance value, so you could say that at that level the value is not significantly differnt than zero. 
So you can find a discount value that after 4 steps, meaning raised to the power of 4, would be around 5% or 0.05. This happens at values lower than 0.47. (0.47^4 = 0.049).
Let's say that we want to predict no more than 6 months into the future. For that, a value of 0.6 discount factor will be appropriate since 0.6^6 = 0.046656. 

```{r}
# GET DLV DATASET

state_levels <- c("New", 'Non'  , 'monthly1' , 'monthly10', 'monthly25', 'monthly50+', 'weekly1' , 'weekly10' , 'weekly25' , 'weekly50+' , 'daily1', 'daily10', 'daily25', 'daily50+')

# Here is a little function to make sure that all states are present in a summary dataframe, and if not - add the missing states and 0 as the amount. 

complete_states <- function(avg_amounts, states_levels) {
  statesdf <- data.frame(states_levels)
  colnames(statesdf) = "states"
  states_amounts <- merge(statesdf, avg_amounts, all.x=T)
  states_amounts[is.na(states_amounts)] <- 0
  states_amounts 
  }

### General Function with arbitrary (1) discount factor and state columns
generate_DLV_dataset_beta <- function(dataframe = df, 
                                         discount_factor = 0.3, 
                                         curr_states_column = "state", 
                                         prev_state_column = "state_prev", 
                                         stateslist = state_levels,
                                         amount_column = "avg_amount",
                                         factorize_df = F) {
    
   # Flow amounts: Avg monthly Amounts driven per state for all drivers in each state in all times
    avg_amounts = aggregate(dataframe[[amount_column]], by=list("states" = dataframe[[curr_states_column]]), FUN=mean)
    avg_amounts[["states"]] <- factor(avg_amounts[["states"]], levels=stateslist, ordered=TRUE)
    
    # if there are missing states that time period, add it and assign 0 with custom function above
    avg_amounts <- complete_states(avg_amounts, stateslist) 

    # Calculate transition Matrix
    trans_matrix <- f_compute_transitions_as_matrix(dataframe, curr_states_column, prev_state_column, factorize = factorize_df )
    
    # Calculate Customer Lifetime Value Solution! 
    ## Getting "A" Matrix: Re-factoring Transition Matrix P to A = (I-dp) = I - discount_factor*trans_matrix
    # get Identity Matrix with the number of dimensions as states:
    I = diag(nrow(trans_matrix)) 
    ## SOLVE Ax=B : EQUATION IS OF THE FORM: Ax = b where A:(I-dP), b = avg_amounts
    A = I - discount_factor * trans_matrix
    # take our constant as average amounts over desired period , as numeric vector
    b = as.vector(avg_amounts[['x']]) 
    # X_Values_named = matrix(data = Xs, ncol = 1, dimnames = list(stateslist))
    DLV_solutions = solve(A, b) 
    DLV_df <-  data.frame(data = cbind(avg_amounts, DLV_solutions))
    DLV_df['DLV_future_remainder'] <- DLV_df$data.DLV_solutions - DLV_df$data.x
    colnames(DLV_df) = c("state","Avg_Amounts_monthly", 
                         "DLV_Values","DLV_future_remainder")
    DLV_df$state <- factor(DLV_df$state, levels = stateslist)
    DLV_df[order(DLV_df[["state"]]),]
    DLV_df
}

### Using the General Function
#df <- setDF(df)
DLV_0.6 <- generate_DLV_dataset_beta(dataframe = df, discount_factor = 0.6)
DLV_0.6_2015 <- generate_DLV_dataset_beta(dataframe = setDF(dt_2015), discount_factor = 0.6)
DLV_0.4_2015 <- generate_DLV_dataset_beta(dataframe = setDF(dt_2015), discount_factor = 0.4)
DLV_0.4 <- generate_DLV_dataset_beta(dataframe = setDF(dt), discount_factor = 0.4)

DLV_0.3_early <- generate_DLV_dataset_beta(dataframe = setDF(dt[date<as.Date("2015-07-02")]), discount_factor = 0.3)
DLV_0.3_early
```




# Prediction Power Check: Histograms of true and estimated DLVs per state 
*Comparing Markov Predicted DLV with actual amounts data.*
As a methodological check for confidence, we can simply report statistical significance or confidence intervals. However, these are somewhat not useful in terms that they don't show us clearly the underlying distributions of the populations nor of the predictions and where does our predicted value or the true value sit on them; which would be a much more comprehensive and understandable way to convey our confidence than simple statistics like #confidenceintervals. 




##  Generate TRUE DLV dataset and merge with Predicted DLV Values dataset (2016)

For data manipulation (for raising the discount factor to the power of the number of steps), we need to create a column with the ordinal number of the chronological steps:
```{r}
# since our steps our defined by monthyear, we'll convert it to its ordinal order.
# that is easy in our case since it's already a numerical equivalent which appears in ascending order in our dataset, we can easily convert it to a factor which will be set automatically with ascending levels, and then use the numeric representation of that factor.
dt$monthyear <- factor(dt$monthyear, ordered = TRUE)
dt$timestep <- as.numeric(dt$monthyear)
df <- setDF(dt)
```


# Visualizing Historical Values
```{r}

```


```{r}
require("lubridate")

generate_donor_DLVs_predictions <- function(df, discount, DLV_dataset, states_column = "state", 
                                             state_levels=state_levels, startdate="2015-05-01") {
    require("lubridate")
    
   # First, since we are only predicting for the donors/users who started at that started, 
   # let's filter out data about other donors/users that were not present then. 
    donors_startdate <- unique(df$donor_uid[df$date == startdate])
    df_startdatedonors <- subset(df, df$date >= startdate & donor_uid %in% as.vector(donors_startdate))
    
    # map each donor / user to its starting state
    original_states_startdate <- df_startdatedonors[df_startdatedonors$date == 
                                                      startdate,c("donor_uid",states_column)]
    colnames(original_states_startdate) <- c("donor_uid","starting_state")
    
    # Merge to create a new dataset of donors with original states column
    df_startdatedonors <- merge(x = df_startdatedonors,
                                       y = original_states_startdate, 
                                by = "donor_uid", all.x = TRUE)
    
    # create a week # column (for later raising discount factor by that power)
    ### THIS ONLY WORKS IF INSPECTED IS 01/01 
    df_startdatedonors$discount_factor <- discount ^ df_startdatedonors$timestep # Discount factor per week 
    # discounted amounts
    df_startdatedonors$discounted_week_amounts <- (df_startdatedonors$avg_amount 
                                                        *  df_startdatedonors$discount_factor)
    # aggregate amounts per donor
    true_dlvs <- ddply(df_startdatedonors, .(donor_uid), 
                       summarise, true_dlv_amounts=sum(discounted_week_amounts))
    # merge into dataset of donor_uid, donor_original_state, sum_amounts
    donors_DLVs_bystate <- merge(x = original_states_startdate,
                                  y = true_dlvs, by = "donor_uid", all.x = TRUE)
    # add column of mean true value per state
    donors_DLVs_bystate <- ddply(donors_DLVs_bystate, "starting_state", 
                                  transform, true_dlv_mean  = mean(true_dlv_amounts))
    
    ## COMBINE WITH MARKOV ESTIMATED VALUES: ACCORDING TO PRIOR UNSEEN DATA 2015
    # add predicted values per state
    donors_DLVs_all <-  merge(x = donors_DLVs_bystate, 
                               y=DLV_dataset[,c("state",unlist(grep('DLV_Value',
                                                                    colnames(DLV_dataset),
                                                                    value=TRUE)))], 
                               by.x = 'starting_state', by.y = 'state', all.x = TRUE)
    donors_DLVs_all$starting_state <- factor(donors_DLVs_all$starting_state, 
                                                     levels = state_levels, ordered = T)
    return(donors_DLVs_all)
}

# run function: generate these datasets for needed disocunt factors 
donors_DLVs_combined_0.6 <- generate_donor_DLVs_predictions(df, discount=0.6, DLV_dataset=DLV_0.6) 
fwrite(donors_DLVs_combined_0.6, "./data/donors_DLVs_combined_0.6.csv") # save datasets

donors_DLVs_combined_0.4_2015 <- generate_donor_DLVs_predictions(df, discount=0.4, DLV_dataset=DLV_0.4_2015) 
fwrite(donors_DLVs_combined_0.4_2015, "./data/donors_DLVs_combined_0.4_2015.csv")

donors_DLVs_combined_0.3_2015 <- generate_donor_DLVs_predictions(df, discount=0.3, DLV_dataset=DLV_0.3_2015) 
fwrite(donors_DLVs_combined_0.3_2015, "./data/donors_DLVs_combined_0.3_2015")

### others:

donors_DLVs_combined_0.6_2015 <- generate_donor_DLVs_predictions(df, discount=0.6, DLV_dataset=DLV_0.6_2015, startdate = "2015-05-01") 
fwrite(donors_DLVs_combined_0.6_2015, "./data/donors_DLVs_combined_0.6_2015")

donors_DLVs_combined_0.4 <- generate_donor_DLVs_predictions(df, discount=0.4, DLV_dataset=DLV_0.4) 
fwrite(donors_DLVs_combined_0.4, "./data/donors_DLVs_combined_0.4.csv")

# show result
head(donors_DLVs_combined_0.6)
head(donors_DLVs_combined_0.3_2015)
```

**** do :
donors_DLVs_combined_0.4 <- generate_donor_DLVs_predictions(df, discount=0.4, DLV_dataset=DLV_0.4) 
fwrite(donors_DLVs_combined_0.4, "./data/donors_DLVs_combined_0.4.csv")


Let's see the true DLVs means are relative to our predictions:
```{r}
setDT(donors_DLVs_combined_0.6)
colnames(donors_DLVs_combined_0.6) <- c("starting_state", unlist(colnames(donors_DLVs_combined_0.6)[2:5]))
unique(donors_DLVs_combined_0.6[ ,.(starting_state,true_dlv_mean, DLV_Values)])

```


# Visualizations
Now we got all our necessary datasets in place for both communicating the values of which directly (like the transition matrix) and to create the needed visualizations to communicate the trends effectively.
Let's start creating those visualizations.


## How far are we currently from the Steady State?
Compare Steady state to true donor allocation
```{r}
require(scales, ggplot2, ggthemes)

state_levels <- c("New", 'Non'  , 'monthly1' , 'monthly10', 'monthly25', 'monthly50+', 'weekly1' , 'weekly10' , 'weekly25' , 'weekly50+' , 'daily1', 'daily10', 'daily25', 'daily50+')


plot_steady_state_and_reality_compared <- function(fulldataframe = df, state_levels = state_levels, states_column = "state", steadystates = steady, analysis_title="") {

  # DF of donor proportions
  df_donor_props <- summary(fulldataframe[[states_column]]) / nrow(fulldataframe)
  
  df_donor_props_vertical <- t(rbind(steadystates, df_donor_props))
  colnames(df_donor_props_vertical) <- c("Steady_State", "Reality")
  melted_donors_props <- melt(df_donor_props_vertical)
  colnames(melted_donors_props) <- c("state","scenario","proportion")
  melted_donors_props$proportion <- as.numeric(as.character(melted_donors_props$proportion))
  melted_donors_props$state <- factor(melted_donors_props$state, levels=rev(state_levels), ordered = TRUE)
  melted_donors_props[order(melted_donors_props$state),]
  
  # PLOT 
  gg_steady_state_and_reality_compared <- ggplot(data=melted_donors_props, 
                                                 aes(x=state, y=proportion, 
                                                     fill=scenario)) +
    geom_bar(stat="identity", position=position_dodge(0.8)) + 
    coord_flip() + 
    scale_fill_discrete(name = "Scenario: ", labels = c("Predicted Steady State      ", "Reality (2015-2016 Avg)"), h =  c(10,170)) +
    labs(x = "Donor State (in Steady State vs Reality)", y = "Proportion of Donors", 
         title = paste0(analysis_title, "Donor Proportions in Stedy State / Reality")) +
    theme_minimal()  + theme(legend.position = "top") +
    #scale_x_continuous(limits = c(0,0.6)) +
    theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
          plot.subtitle = element_text(family = "Helvetica"))  + 
    scale_fill_ptol() 
  
  ggsave("./dataviz/gg_steady_state_and_reality_compared.png", height = 6, width = 9)
  return(gg_steady_state_and_reality_compared)
}

gg_steady_state_and_reality_compared <- plot_steady_state_and_reality_compared(fulldataframe=df, state_levels=state_levels)
gg_steady_state_and_reality_compared

```

* Notice that the barplot above has a follows important visualization principles: 
- The bars are horizontal rather than vertical, since the list of categories is long and they would be too compressed and unreadable if they were to be aligned horizontally. 
- The colors are contrasting colors from the ptol() scheme which is friendly for the eye yet considering color blindness. 
- The legend is up at the top and conveniently tells us exactly all we need to know.
- The bars are arranged according to the manually specified order: from the categories with the least donations to most contribution, which also roughly corresponds with the highest amounts of donors to least (inverse correlation)




## Visualize: DLVs donor Lifetime Values as stacked bar plots

## Constant: donor LifeTime Value

Now we want to see the donor lifetime values. Since this depends on discount factors, let's visualize the donor's lifetime value according to various discount factors and be able to compare between them. If the ranking or proportions stays the same, then that would be simple for us and indicate stability of ranking between profitability of states regardless of time horizon.
For that, let's first generate another dataset of discounted lifetime values:
```{r}
#DLV_0.8 <- generate_DLV_dataset_beta(dataframe = df, discount_factor = 0.8)
```

And now plot on the same plot both current donation amounts, and 2 levels of discounted predicted lifetime values.
```{r}

plot_DLV_stacked_barplot_0.6_0.8 <- function(DLV_0.6,DLV_0.8, beta_1=0.6, beta_2=0.8, analysis_title="") {
    library("ggthemes")
  
    DLV_0.6_08 <- cbind(DLV_0.6,DLV_0.8[c(grep('Value', colnames(DLV_0.8), value=T),
                                            grep('remainder', colnames(DLV_0.8), value=T))])
    colname_DLV_value_1 <- paste0(beta_1,"_DLV_Value")
    colname_DLV_value_2 <- paste0(beta_2,"_DLV_Value")
    colname_DLV_remainder_1 <- paste0(beta_1,"_DLV_remainder")
    colname_DLV_remainder_2 <- paste0(beta_2,"_DLV_remainder")
    
    colnames(DLV_0.6_08) <- c("state", "Avg_monthly_Amounts", colname_DLV_value_1,
                              colname_DLV_remainder_1 , colname_DLV_value_2, colname_DLV_remainder_2)
    DLV_0.6_08_usable <- DLV_0.6_08[,c("state","Avg_monthly_Amounts", 
                                       colname_DLV_remainder_1, colname_DLV_remainder_2)]
    # Melt Dataframe to Long format for ggplot
    melted_dlv <- melt(DLV_0.6_08_usable,id.vars = "state") 
    melted_dlv$variable <- factor(melted_dlv$variable, 
                           levels=c(colname_DLV_remainder_2, colname_DLV_remainder_1,"Avg_monthly_Amounts"), ordered=TRUE)
    
    # Plot stacked bar plot
    gg_stacked_barplot_DLV_amounts_monthly <- ggplot(data = melted_dlv, aes(x = state, y = value,fill=variable)) +
       geom_bar(stat = 'identity') + 
       coord_flip() + scale_x_discrete(limits = rev(levels(melted_dlv$state))) + 
       labs(title = paste0(analysis_title, "Donors Current Donations and Lifetime Values"),
            subtitle = paste0("Discount factors = ",beta_1," and ",beta_2,
                              ". \nDLV: Predicted total discounted lifetime value"),
            y = "Value (Discounted Amounts)", x = "State", fill = "DLV Value Portion   ") +
        theme_fivethirtyeight() + 
        theme(legend.position = "top") +
        guides(fill = guide_legend(reverse=T)) +
        scale_fill_brewer(palette = 15) # scale_fill_ptol() +

    ggsave("./dataviz/gg_stacked_barplot_DLV_amounts.png", width = 10, height = 7)
    gg_stacked_barplot_DLV_amounts_monthly
}

plot_DLV_stacked_barplot_0.6_0.8(DLV_0.6, DLV_0.8, beta_1=0.6, beta_2=0.8)


```



## Visualize Model Prediction Accuracy: Histograms of true DLVS distributions and where estimated DLVs fall

### DLVS Distributions per State: Arbitrary Beta Function 

*Comparing Markov Predicted DLV with actual amounts data.*
As a methodological check for confidence, we can simply report statistical significance or confidence intervals. However, these are somewhat not useful in terms that they don't show us clearly the underlying distributions of the populations nor of the predictions and where does our predicted value or the true value sit on them; which would be a much more comprehensive and understandable way to convey our confidence than simple statistics like #confidenceintervals. 

```{r}
#donors_DLVs_combined_0.6_2015 <- generate_donor_DLVs_predictions(df, discount=0.6, DLV_dataset=DLV_0.6_2015) 
setDT(donors_DLVs_combined_0.6)
colnames(donors_DLVs_combined_0.6) <- c("starting_state", unlist(colnames(donors_DLVs_combined_0.6)[2:5]))
unique(donors_DLVs_combined_0.6[, c("starting_state", "true_dlv_mean")])

# save datasets
#fwrite(donors_DLVs_combined_0.4_2015, "./donors_DLVs_combined_0.4_2015.csv")
#fwrite(donors_DLVs_combined_0.6_2015, "./donors_DLVs_combined_0.6_2015.csv")
```

Now that we have the needed dataset of true DLV values, let's visualize their distrubtions per state, and our predicted state DLV.

```{r}

plot_DLV_preds_dists <- function(DLV_dataset, discount, data_timeframe_str = "All data", analysis_title="") {
      names(DLV_dataset)[names(DLV_dataset) == grep('DLV_Value', colnames(DLV_dataset), value=T)] <- "DLV_Values"
      names(DLV_dataset)[names(DLV_dataset) == grep('true_dlv_mean', colnames(DLV_dataset), value=T)] <- "true_dlv_mean"
      errors =  abs(DLV_dataset[['DLV_Values']] - DLV_dataset[['true_dlv_mean']]) 
      gg_dists <- ggplot(DLV_dataset, aes(x=true_dlv_amounts), group = starting_state) +
        
        # histogram of true values per state, y=..density..:
        geom_histogram(aes(y=..density..), position="identity", fill = 'steelblue', alpha=0.5,  binwidth=5) +
        
        # density - overlay with transparent density plot:
        geom_density(alpha=0.2, color = 'darkgrey' ) +
        
        # true values mean:
        geom_vline(data=DLV_dataset, aes(xintercept = true_dlv_mean), 
                   color = 'gold', linetype="dashed", size=1, show.legend=T) +
        
        # estimated markov value:
        geom_vline(data=DLV_dataset, aes(xintercept = DLV_Values), 
                   color='red',linetype="dashed", size=1, show.legend=T) +
        facet_wrap( ~ starting_state,  scales = "free") +
        labs(x = "Total DLV (Amounts)", y = "Density", 
             title = paste0(analysis_title, "Distributions of True DLV Amounts with Predicted DLV value, Discount Factor = ", 
                            discount, ", trained on ", data_timeframe_str),
             subtitle = paste0("What was the true total DLV amounts donated of drivers who started off in 04/2015 at each of the following states?
Blue: Distributions of their individual DLVs, Yellow: Group true mean, Red: Markov Model estimated value. 
Mean Error: ", round(mean(errors),2), " . Median error: ", round(median(errors),2), " over the true values") ) 
      ggsave(filename = paste0("./dataviz/gg_DLV_states_distributions__",discount,"_", gsub('([[:punct:]])|\\s+','_',data_timeframe_str) ), 
             plot = gg_dists, device = "png", width = 10, height = 7 )
      gg_dists
}


# Plot!
plot_DLV_preds_dists(donors_DLVs_combined_0.6_2015, discount = 0.6, 
                                      data_timeframe_str = "only 2015 data")



```

## Performance evaluation
The above visualization shows the distributions of true DLVs of all drivers, their mean in yellow, and our predicted DLV in red. 
What can we do about it?
1. The simplest thing is to continue as is and know to discount our prediction by a factor of roughly a half.
2. We can try to improve our model inherently. For example, instead of using 1-step markov process, we can look further in the back in order to predict, in a 2-step (or more) Markovian process, or redefine the quantity used for prediction to account for the past more.
3. We can use ensemble models: given our current model's results as predicted y and the true y labels, we can overlay another model, perhaps a simple regression, to learn how to turn one result to the other. This could have promising potential if our errors are consistent. We could use the factor(state) as a regressor (which is equivalent to one-hot encoding of each state as regressors), the predicted y, and the true y as a label. This would be a "fixed effects" regression model. By using this model, we could significanly improve the results.
However, this doesn't yet fix the inherent results of the model, which is preferable if we can.
4. Since we see *consistent overestimation* of our prediction, there might be a simple fix: it might just be fixed via a proxy of callibrating the discount factor to generate the results we want. 
Let's check prediction results of other discount factors and their correlation with our desired true y's (DLV amounts with a 0.6 discount factor).

```{r}
# we need the true DLVs with 0.6 discount factor, 
# and the predictions for 0.4 values (or similar),
# and see if they match better. 
# if they do, we can just change our reference point of discount factor to fit reality. 
preds_dt <- donors_DLVs_combined_0.3_2015
true_dlvs_dt <- donors_DLVs_combined_0.6
f_combine_pred_true_dts_2discounts <- function(preds_dt, true_dlvs_dt) {
  setDT(true_dlvs_dt)
  setDT(preds_dt)
  # renaming columns for compatibility
  # colnames(preds_dt) <- c("starting_state", unlist(colnames(preds_dt)[2:5]))
  DLVs_pred_train_unique <- unique(preds_dt[, .(starting_state, true_dlv_mean, DLV_Values)])
  DLVs_trues_unique <- unique(true_dlvs_dt[, .(starting_state, true_dlv_mean, DLV_Values)])
  
  # change column names to include the name / type of dataset before merging
  colnames(DLVs_pred_train_unique) <- c("starting_state", "pred_dlvs_true_train", 
                                      "dlv_pred_train")
  colnames(DLVs_trues_unique) <- c("starting_state", "true_dlvs", 
                                 "true_dlvs_pred")
  setDF(DLVs_pred_train_unique)
  setDF(DLVs_trues_unique)
  # merge pred train predictions and trues all true values
  #setkey(DLVs_pred_train_unique, starting_state)
  #setkey(DLVs_trues_unique, starting_state)
  DLV_preds_predtrain_truesall <- merge(DLVs_pred_train_unique[c("starting_state", 
                                                                 "dlv_pred_train")], 
                                     DLVs_trues_unique[c("starting_state", "true_dlvs")])
  DLV_preds_predtrain_truesall
}

# run function: merge predictions and true datasets of different discount factors.
DLV_preds_0.4train_0.6all <- f_combine_pred_true_dts_2discounts(donors_DLVs_combined_0.4_2015, 
                                                                donors_DLVs_combined_0.6)
DLV_preds_0.4train_0.6all
```

This is still about half but still overshooting. Let's try a lower discount value.
```{r}
# DLV_0.3_2015 <- generate_DLV_dataset_beta(dataframe = setDF(dt_2015), discount_factor = 0.3)
# donors_DLVs_combined_0.3_2015 <- generate_donor_DLVs_predictions(df, discount=0.3, DLV_dataset=DLV_0.3_2015) 

# fwrite(donors_DLVs_combined_0.3_2015, "./data/donors_DLVs_combined_0.3_2015")
# donors_DLVs_combined_0.3_2015
```



```{r}
# run function: merge predictions and true datasets of different discount factors.
#DLV_0.4_2015 <- generate_DLV_dataset_beta(dataframe = setDF(dt_2015), discount_ factor = 0.4)
colnames(donors_DLVs_combined_0.3_2015) <- c("starting_state", colnames(donors_DLVs_combined_0.3_2015)[2:5])
DLV_preds_0.3train_0.6all2 <- f_combine_pred_true_dts_2discounts(preds_dt = donors_DLVs_combined_0.3_2015, 
                                                                 true_dlvs_dt = donors_DLVs_combined_0.6)
DLV_preds_0.3train_0.6all2

```

Let's check which is better correlated
```{r}
print(cor(DLV_preds_0.3train_0.6all2$dlv_pred_train, DLV_preds_0.3train_0.6all2$true_dlvs))
print(cor(DLV_preds_0.4train_0.6all$dlv_pred_train, DLV_preds_0.4train_0.6all$true_dlvs))

```
It looks like our first model is slightly better correlated, but they are actually both very well correlated.
How about which is actually closer? let's calcualted the sum of errors / squared errors:
```{r}
install.packages("caret")
library(caret)
print(caret::postResample(pred = DLV_preds_0.3train_0.6all2$dlv_pred_train, obs = DLV_preds_0.3train_0.6all2$true_dlvs))

print(caret::postResample(pred = DLV_preds_0.4train_0.6all$dlv_pred_train, obs = DLV_preds_0.4train_0.6all$true_dlvs)
)
 
```

So we see that the R square is high and comparable in both cases (0.96 or 0.97), where the RMSE is greater for the 0.4 discount factor rather than the 0.3 discount factor. We will go with the 0.3 discount factor for now as our proxy estimation for what a 0.6 discount factor will be.


## Reiterating Model Performance: Adjusted Discount Factor.
Now let's check the performance under the adjusted discount factor; translating 0.6 to 0.3 discount factors. 
Let's create that matched combined dataset with 0.6 true DLVs and predictions using 0.3:
```{r}
# take the predcitions from here: DLV_Values
# I've made the plotting function take as the prediction value any column with "DLV_Value" in its name
# so I'm going to keep that in the name, and remove that part of the name from the other one.
colnames(donors_DLVs_combined_0.3_2015) <- c(colnames(donors_DLVs_combined_0.3_2015)[1:4], "DLV_Value_Preds_0.3_train")

# and copy into true DLV values (amounts and means) from here - let's rename it so it would be clearer and not treated as predictive DLV Values
colnames(donors_DLVs_combined_0.6_2015) <- c(colnames(donors_DLVs_combined_0.6_2015)[1:4], "old_DLVs_0.6_train")
# we'll set both DT keys as donor_ids and then match on that. 
setkey(donors_DLVs_combined_0.3_2015, donor_uid)
setkey(donors_DLVs_combined_0.6_2015, donor_uid)
dt_DLVs_preds <- merge(donors_DLVs_combined_0.6_2015, donors_DLVs_combined_0.3_2015[,c("donor_uid","DLV_Value_Preds_0.3_train")])
dt_DLVs_preds
```

Plotting the distriubitons of true values and predicted value on top of those.s
```{r}
# now let's plot again using our previous function for plotting
plot_DLV_preds_dists(dt_DLVs_preds, discount = "(0.3) Matching 0.6", data_timeframe_str = "only 2015 data")

```


This is much better!
Now we see that actually the predicted DLV Values (red dashed lines) are pretty close to the true observed population mean (yellow dashed line) and always within a central area of the true data distribution. This all was achieved only by this very simply 1-step markovian model, without any model ensembling or complications of the model itself; but just with an adjustment of the discount factor reference point itself.


# Visualizing Historical Running Values


## Prep: add date column
We mostly had a "monthyear" column that was meant to just convey order until now. Now we need an actual date column for plotting dates smoothly on the X axis by ggplot, that not all datasets have. 
```{r}
# let's create a function that converts month-year column to a date column
convert_monthyear_to_date <- function(any_df) {
  any_df[['date']] = as.Date(paste0(any_df$monthyear, "-01") )
  any_df$monthyear <- NULL
  any_df
}
convert_monthyear_to_date(monthly_amount_avgs)
convert_monthyear_to_date(monthly_diagonals)
convert_monthyear_to_date(monthly_trans_from_dailylow)
```


## Donations Flow: Monthly Average Donations Amounts

```{r}
### Flow: monthly Amount Average
require(directlabels)

plot_monthly_amount_flows <- function(df_monthly, state_levels = state_levels, analysis_title="") {
    
    df_monthly$state <- factor(df_monthly$state, levels = state_levels)
    # drop the last week which is incomplete
    # df_monthly <- df_monthly[df_monthly$date != max(df_monthly$date),]
    # df_monthly <- df_monthly[df_monthly$date != min(df_monthly$date),]
    gg_monthly_flow_amount_avg <- 
      ggplot(data = df_monthly, aes(x = date, y = avg_amount, color=state)) +
      #geom_point(size=0.8) +
      geom_line(size=0.8, alpha = 0.6)  + 
       labs(title = paste0(analysis_title, "History of Average Monthly Donations From Each State"), 
        subtitle = "What was the average donation by donors in each state each month?",
        y = "Average Monthly Donation in $USD", x = "Month (2015-2016)", color = "Donor State  ") +
        geom_dl(aes(label = state), method = list(dl.trans(x = x + 0.1), "last.points", cex = 0.6)) +
        geom_dl(aes(label = state), method = list(dl.trans(x = x - 0.1), "first.points", cex = 0.7)) +
        theme_minimal() +
        theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
              plot.subtitle = element_text(size=12, family = "Helvetica"))  +

    ggsave(filename = "./dataviz/gg_monthly_flow_amount_avg", device = "png", width = 10, height = 6)
    gg_monthly_flow_amount_avg
  }

plot_monthly_amount_flows(df_monthly= setDT(monthly_amount_avgs)[state!='New',], state_levels = state_levels) # ORIGINAL 

```

Great. The above plot shows us that the actual donations per state didn't change much through time. Almost all states stayed relateively constant throughout the whole time, while only Weekly and Daigly high amount donors varied more widely.
This makes sense from two reasons:
1. Firstly it is a result of the way I segmented the states: that state includes all donation amounts above $50 while the others have a very limited range of up to 25 USD max, so this state has much more natural variance.
2. This state has fewer donors, thus small variations in 1 or 2 donors would lead to a high variation in the average value of the state. 

Seeing that this is acceptable, let's move on.


### Number of donors per state throughout time
```{r}
### Number of donors per state throughout time
generate_num_donors_groups <- function(df, states_column="state") {
    library(plyr)
    num_donors_groups <- ddply(.data = df[c("donor_uid","date",states_column)],
                                    .variables = .(date, eval(parse(text=states_column))), .fun = nrow)
    colnames(num_donors_groups) <- c("date","state","Num_donors")
    return(num_donors_groups)
}

plot_donor_groupsize_timeline <- function(num_donors_groups, analysis_title = '') {
    
    ### Number of donors per group throughout time history
    gg_donors_groupsizes_timeline <- 
      ggplot(data = num_donors_groups, aes(x = date, y = Num_donors, color=state)) +
       geom_line(size=1, alpha = 0.7)  +
       labs(title = paste0(analysis_title = '',"Monthly Number of Donors In Each State"), 
        subtitle = "Each month, how many donors were there in each state (group)?",
        y = "Number of Donors (in State S that Month)", x = "Month", color = "Donor State") +
        theme_minimal()  + 
        geom_dl(aes(label = state), method = list(dl.trans(x = x + 0.1), "last.points", cex = 0.6)) +
        geom_dl(aes(label = state), method = list(dl.trans(x = x - 0.1), "first.points", cex = 0.7)) + 
        theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
              plot.subtitle = element_text(family = "Helvetica"))  
    ggsave(filename = "./dataviz/gg_donors_groupsizes_timeline", device = "png", width = 10, height = 6)
    gg_donors_groupsizes_timeline
}

# generate neeeded dataset
num_donors_groups <- generate_num_donors_groups(setDF(df))

# plot the timeline!
gg_donors_groupsizes_timeline <- plot_donor_groupsize_timeline(num_donors_groups)
gg_donors_groupsizes_timeline
```

This group sizes plot reveals these main conclusions:
* There are many more new donors along the data (by the decrease in the "Non" count)
* Monthly donors join the most, and there are less donors joining / sustaining as more frequent the group gets
* There are more donors joining or sutstaining the lower the donation value is.
All in all, it is quite an intuitive conclusion here that there are more donors the lower the commitment is: in amount or frequency. However, this reveals an important conlcusion: that frequency matters more than amount; since there are more monthly high amount donors rather than weekly or daily low amount donors. Perhaps a high frequency means more "commitment" for most people in our sample, which would be an interesting area of investigation. 

## Transition Probabilities History: Values on the Diagonal (repeating states) every week
```{r}
# Transition Matrix Throughout Time: Values on the Diagonal (transition to self) every week
plot_transition_matrix_monthly_history <- function(monthly_df, titletext, 
                                                   analysis_title = '', subtitletext='', melt_it=T) {
  # if the dataset is a matrix, melt it first
  if (melt_it) {monthly_df <-  melt(monthly_df, id.vars = "date")}
  colnames(monthly_df) <- c("date", "state", "transition_prob")
  gg_transitions_matrix_timeline <-
    ggplot(data = monthly_df, aes(x = date, y = transition_prob, color=state)) +
     geom_line(size=1, alpha = 0.7)  +
     # geom_smooth(method = 'loess', span = 0.4, alpha = 0.1, size=0.3) + 
     labs(title = paste0(analysis_title, "Monthly Transition Probabilities: ",titletext), 
      subtitle = subtitletext,
      y = "Probability of State Repetition", x = "Month", color = "Donor State") +
      theme_minimal()  + 
      geom_dl(aes(label = state), method = list(dl.trans(x = x + 0.1), "last.points", cex = 0.4)) +
      geom_dl(aes(label = state), method = list(dl.trans(x = x - 0.1), "first.points", cex = 0.5)) + 
      theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
            plot.subtitle = element_text(family = "Helvetica"))  #+ scale_colour_ptol() 
  ggsave(filename = "./dataviz/gg_transitions_matrix_timeline", device = "png", width = 10, height = 6)
  gg_transitions_matrix_timeline
}
# clear out empty column with no transitions to NEW or repetitions of NEW by definitions
# monthly_diagonals <- convert_monthyear_to_date(monthly_diagonals)
# monthly_diagonals$New <- NULL
# Plot DIAGONAL values history
plot_transition_matrix_monthly_history(monthly_df = monthly_diagonals, 
                                       titletext = "Repeats", 
                                       subtitletext = "Probability Of Repeating The Same State",
                                       melt_it = T)
```


So the above plot shows us a few things:
1. Most transitions are relatively stable, nut moving much more than 10% in their history (weekly and monthly states)
2. Some states had huge fluctations, particularly daily25, moving from 0 to 1 in September 2015 and going back towards zero. How can this be? this could very well come from a too small sample. Let's look if that's the case:

```{r}
setDT(dt)
dt[state=="daily25" & date>=as.Date("2015-08-30") & date<as.Date("2015-10-02") ,]
```

Yes, indeed, by filtering we see that there were too few donors making that transition at that time to really say that it's logical to have a 100% transition probability between those states.
Let's drop transitions for numbers of months with fewer than 10 donors:

```{r}
setDT(num_donors_groups)
num_donors_groups[Num_donors<10]
```
Sadly, these are 19 combinations (out of 124) that we can't make a sufficient conclusion for.
Let's drop those from our matrices:
```{r}
remove_smallsample_transitions <- function(monthly_matrix, num_donors_groups) {
  matrix_melted <- melt(monthly_matrix, id.vars = .(date))
  colnames(matrix_melted) <- c("date", "state", "transition_prob")
  # num_donors_toosmall <- num_donors_groups[Num_donors<10, ]
  num_donors_enough <- num_donors_groups[Num_donors>=10, ]
  # left join for only rows of num_donors_enough:
  matrix_melted_large <- merge(num_donors_enough[, c("date", "state")], matrix_melted, all.x = TRUE)
  matrix_melted_large <- matrix_melted_large[complete.cases(matrix_melted_large), ]
  matrix_melted_large
}

monthly_diagonals_largeNs <- remove_smallsample_transitions(monthly_matrix = monthly_diagonals, num_donors_groups)
monthly_diagonals_largeNs
trans_matrix <- spread(as.data.frame(monthly_diagonals_largeNs), key =  state, value = n, fill = 0,  drop = FALSE) 

```

Now we have only more credible transitions.
Let's see the transitions output of only more credible transitions?
```{r}
plot_transition_matrix_monthly_history(monthly_df = monthly_diagonals_largeNs, 
                                       titletext = "Repeats", 
                                       subtitletext = "Probability of repeating the same state",
                                       melt_it = F)
```


Now we have the more credible transitions. While the transitions are still not very stable, there are at least no nonsensible leaps from 0 to 1 and back. The fact that the transitions are very unstable shows that the states themselves are not very stable.
According to the transitions matrix, we can see that most donors are normally on "Non", or a low frequency donation mode, and make one donations at a given time and then return back to their less active state. This can explain why are these transitions erlatively low. 
It is very interesting to see that in September there was a peak in state repetition probability meaning that most donors repeated their own state, and then a drop. There was such a peak again in December and then a drop in January. It might be a consequence of political events or campaigning efforts. 
One striking differnce over time is in the "daily" states - they really climbed in value since they entered the dataset, meaning that people got more and more consistent in giving. 

### DLV VALUES throughout time

```{r}
### DLV VALUES throughout time

### 
plot_dlv_values_timeline <- function(DLV_monthly = DLV_monthly_0.6, state_levels = state_levels, analysis_title = '') {
  names(DLV_monthly) <- c("date", "state", "avg_amount_monthly", "DLV_Values", "DLV_future_remainder")
  DLV_monthly$state <- factor(DLV_monthly$state, levels = state_levels)
  DLV_monthly$date <- as.POSIXct(DLV_monthly$date)
  
  gg_monthly_DLVs <- 
    ggplot(data = DLV_monthly, aes(x = date, y = DLV_Values, color=state)) +
     geom_line(size=1, alpha = 0.7) + 
     labs(title = paste0(analysis_title, "Monthly Estimated Average DLVs Per Donor State"), 
      subtitle = "DLV monthly predictions by the Markov Model, estimated with a 0.3 Discount Factor (equivalent to 0.6)",
      y = "DLV - Discounted donor Lifetime Values (in Discounted Amounts)", x = "Month", color = "Donor State ") +
      geom_dl(aes(label = state), method = list(dl.trans(x = x + 0.1), "last.points", cex = 0.6)) +
      geom_dl(aes(label = state), method = list(dl.trans(x = x - 0.1), "first.points", cex = 0.6)) +
      theme_minimal() +
      theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
            plot.subtitle = element_text(family = "Helvetica"))  # + scale_colour_ptol() 
  ggsave(filename = "./dataviz/gg_monthly_DLVs", device = "png", width = 10, height = 6)
  return(gg_monthly_DLVs)
}

# Generate Monthly DLV Values
# Running monthly DLV estimations and saving all to one dataframe 
DLV_preds_monthly_0.3 <- ddply(.data = df, .variables = .(date), .fun =  generate_DLV_dataset_beta) # , ...(beta = 0.3)
DLV_preds_monthly_0.3

# Plot normal DLV Values History
plot_dlv_values_timeline(DLV_preds_monthly_0.3)


```


This plot shows the following about the data:
1. There is some non-obvious distribution of the DLVs: monthly high donors donate more in total than weekly and daily high donors. It seems as above 50$, the monthly donors would be more skewed upwards than daily and weekly. 
2. It was possible to consdier only one month's data at a time and according to that get DLV values per only that month that was, albeit not perfectly constant, yet most of them were quite stable!
That's not a bad result for the stability of our DLV estimations; we don't see major shifts in ranking or trends that owuld heavily affect our conclusions. 

#### Conclusions from this plot
*Most importantly, the plot reveals the folloing couple of insights:*
The ranking of groups was relatively consistent, and showed that the highest DLVs were of the groups that donated the most in average amount (all the 50+), and ranking down from then, and inside each such combination of groups, there was an INVERSE relationship with the frequency; monthly donors typically donated more than weekly and they donated more than daily donors of the same average amount tier!

Now, for the final outcome, let's see what would be the overall contribution per group of donors in each state - meaning including the average amount and the number of donors!

### DLV Values group-wide
```{r}
### 

plot_dlv_groupsized_values <- function(DLV_monthly, num_donors_groups, analysis_title='',discount_factor='0.6 output (inserted as 0.3)') {
    
    # merge monthly DLVs with groupsize, multiply these columns
    DLV_monthly_groupsized <- merge(DLV_monthly, num_donors_groups, 
                                   by = c("date","state"))
    DLV_monthly_groupsized['Total_group_DLV'] <- DLV_monthly_groupsized$DLV_Values * DLV_monthly_groupsized$Num_donors
    
  
    ### PLOT ###
    gg_monthly_DLVs_grouptotal <- 
      ggplot(data = DLV_monthly_groupsized, aes(x = date, y = Total_group_DLV, color=state)) +
       geom_line(size=1, alpha = 0.7)  + 
       labs(title = paste0(analysis_title, "Monthly Estimated Total DLVs Aggregated By Donor State Group Size that Month. Discount = ",
                           discount_factor),
        subtitle = "Total DLV monthly estimations by the model, times the number of donors in the group that month",
        y = "DLV - Discounted Lifetime Donations", 
        x = "Month", color = "Donor State ") +
        geom_dl(aes(label = state), method = list(dl.trans(x = x + 0.1), "last.points", cex = 0.6)) +
        geom_dl(aes(label = state), method = list(dl.trans(x = x - 0.1), "first.points", cex = 0.6)) +
        theme_minimal() +
        theme(plot.title = element_text(size=18, family = "Helvetica", face = "bold"), 
              plot.subtitle = element_text(family = "Helvetica")) 
    ggsave(filename = "gg_monthly_DLVs_grouptotal_smoothed", device = "png", width = 10, height = 6)
    return(gg_monthly_DLVs_grouptotal)
}

# num_donors_groups <- generate_num_donors_groups(df)
plot_dlv_groupsized_values(DLV_preds_monthly_0.3[DLV_preds_monthly_0.3$state!='Non',], num_donors_groups)

```



Since the numbers of monthly donors increased through time, especially of smaller donations, we would expect an increase in their total contribution. However that didn't account for the total increase. The ranking between which monthly group had higher values fluctuated and switched a few times, and ended roughly on the order by the magnitude of the donation. 
This plot shows the important conclusion that *most of the lifetime value comes from the monthly donors*, even if they donate a small amount, because they were a much larger group. 
Therefore, for the Sanders' campaigners - or democratic campaigners - you might be better off convincing more people to donate a monthly donation of whatever magnitude they can afford, than trying to get a higher frequency of donations that will later be stopped, or than trying to push hard for a higher amount!


---------------------------------------------------------------------------------------------------------
# Conclusion: Key Insights for Decision Makers
To summarize this whole analysis, here are a few of the most important insights that would matter more for decision makers from such an analysis:
- While some of the states are a bit more stable, most states tended to *drop down a tier of frequency but stay at a similar amount of donation the following month*! 
- There were more and more Monthly donors, and more and more of Low donation amounts. Intuitively, it was easy to get people to join or stay in the lower commitment donation patterns. 
- The groups with the highest *average DLV* came from the highest *average donation*, not from frequency. Meaning, even if people donated every day, it still didn't sum up to the same amounts in a whole month as the mothly donors. 
- Within the high amount donors of various frequencies, there was an INVERSE relationship with the frequency; monthly donors typically donated more than weekly and they donated more than daily donors of the same average amount tier!
- In aggregate discounted lifetime donation values (times the number of drivers), *most of the lifetime value comes from the monthly donors*, even if they donate a small amount, because they were a much larger group. Therefore, for the Sanders' campaigners - or democratic campaigners - you might be better off convincing more people to donate a monthly donation of whatever magnitude they can afford, than trying to get a higher frequency of donations that will later be stopped, or than trying to push hard for a higher amount!



